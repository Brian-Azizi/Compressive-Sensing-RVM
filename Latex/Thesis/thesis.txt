Compressive Sensing in Video
Encoding

Brian Azizi
Department of Physics
Centre for Scientific Computing
University of Cambridge

This dissertation is submitted for the degree of
Master of Philosophy

Selwyn College

August 2016

I would like to dedicate this thesis to my loving parents ...

Declaration

I hereby declare that except where specific reference is made to the work of others,
the contents of this dissertation are original and have not been submitted in whole
or in part for consideration for any other degree or qualification in this, or any other
University. This dissertation is the result of my own work and includes nothing which
is the outcome of work done in collaboration, except where specifically indicated in
the text. This dissertation contains less than 65,000 words including appendices,
bibliography, footnotes, tables and equations and has less than 150 figures.
Brian Azizi
August 2016

Acknowledgements

And I would like to acknowledge ...

Abstract

This is where you write your abstract ...

Table of contents
List of figures

viii

List of tables

ix

Nomenclature

x

1 Introduction

1

2 Compressive Sensing

2

3 Wavelets
3.1 Discrete Cosine Transform .
3.2 Wavelet transforms . . . . .
3.2.1 Haar wavelets . . . .
3.2.2 Daubechies Wavelets
3.3 Forming the basis matrix . .

.
.
.
.
.

4
4
4
4
5
5

.
.
.
.
.

6
6
9
11
12
15

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

4 Sparse Bayesian Learning
4.1 Model Specification . . . . . . . . . . . . . . . . . . . .
4.2 Model Inference . . . . . . . . . . . . . . . . . . . . . .
4.2.1 Original Training Algorithm . . . . . . . . . . .
4.2.2 Sequential Sparse Bayesian Learning Algorithm
4.3 Making Predictions . . . . . . . . . . . . . . . . . . . .

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

5 Design of the Multi-Scale Cascade of Estimations Algorithm
17
5.1 Interpolator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
6 Implementation Details and Code optimization
19
6.1 Haar basis functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
6.1.1 1D Haar wavelet transform . . . . . . . . . . . . . . . . . . . . . 19

Table of contents

6.2

6.1.2 2D Haar wavelet transform . . . . . . . . . . . . . . . . . . . . .
6.1.3 3D Haar wavelet transform . . . . . . . . . . . . . . . . . . . . .
Update formulae, details on RVM implementation . . . . . . . . . . . .

vii
20
22
22

7 Results

23

8 Conclusion

25

References

26

List of figures
2.1

Example of a signal pair x (left) and y (right). We wish to reconstruct
x from y. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2

Original image x (left) and its Haar basis transformation w (right). See
next chapter for more details on Haar wavelets. . . . . . . . . . . . . .

5

4.1

Plots of the Marginal Likelihood Function . . . . . . . . . . . . . . . .

13

5.1

ˆ (right) using a
Corrupted signal y (left) and reconstructed signal x
cascade of 3 RVMs with Haar basis functions (see [3]). . . . . . . . . .

18

3.1

Sample
(right)
7.2 Sample
(right)

7.1

frame from corrupted video
. . . . . . . . . . . . . . . .
frame from corrupted video
. . . . . . . . . . . . . . . .

(left) and the reconstructed video
. . . . . . . . . . . . . . . . . . . .
(left) and the reconstructed video
. . . . . . . . . . . . . . . . . . . .

24
24

List of tables

Nomenclature
Roman Symbols
M

Number of basis functions

N

Number of training examples

w

RVM weights vector

x(i)

ith input vector

y (i)

ith target

Greek Symbols
ϕj

jth basis vector

ϕj (·) jth basis function

Chapter 1
Introduction
There are three parts: A signal processing framework called Compressive Sensing
(CS), a pre-processing step in form of a basis transformation based on discrete wavelet
transforms and a Machine Learning algorithm called Sparse Bayesian Learning.
The key notion that ties in these three areas is the notion of sparsity.

Background

Chapter 2
Compressive Sensing
This section is based on [3]. The problem to be solved can be formulated as follows: Let
x ∈ RN be a signal of interest. We do not measure x directly and it is thus unknown.
Instead, we have a measurement y ∈ RM , with M << N , from which we want to
reconstruct x. The signals x and y are related as follows:
Ωx = y

(2.1)

where Ω is a known M × N matrix referred to as the sensing matrix.
For example, in [3], the signal of interest x is an image, so that N is equal to the
total number of pixels in the image and xi is equal to the intensity of the corresponding
pixel. However, we imagine that we have only access to a corrupted version of x in
which random pixel values have been deleted. This is our measurement y. See Figure
?? for an example. The sensing matrix Ω corresponding to this scenario is obtained

Fig. 2.1 Example of a signal pair x (left) and y (right). We wish to reconstruct x from
y.

3
by taking the N × N identity matrix and deleting the rows that correspond to the
missing entries in x.
Compressive Sensing (CS) is a collection of signal processing techniques that allow
for efficient reconstruction (and indeed aquisition) of such signals by solving the
underdetermined system (2.1).
Of course, there are infinitely many solutions to an underdetermined system. In
ˆ that is sparsest in some domain. By
the CS framework, we seek to find a solution x
ˆ that satisfies (2.1), such that there exists a basis
that, we mean that we want to find x
ˆ in which it has the smallest number of nonzero entries.
transformation of x
More concretely, we assume there exists a domain in which the desired signal x is
sparse. I.e. there exists a N × N basis matrix Ψ such that x = Ψw and w is sparse.
The CS problem can then be expressed as follows:
min ||w||0

subject to

ΩΨw = y

where ||.|| denotes the l0 norm, i.e. the number of nonzero components.
For a more detailed review of the CS framework, see [1].

(2.2)

Chapter 3
Wavelets
In this chapter, we will introduce wavelet functions and the Discrete Wavelet Transform
(DWT). The DWT allows us to transform our signal into a new basis in which it is
sparse.
x = Ψw
(3.1)
that takes the dense signal x and sends it into a domain in which its transformation
w = ΨT x is sparse.

3.1

Discrete Cosine Transform

An aside on the DCT. Used in old JPEG standard (ref). Formulae. Interpretations.
Pictures. However, before we discuss wavelets, we will briefly introduce the Discrete
Cosine Transform (DCT)

3.2
3.2.1

Wavelet transforms
Haar wavelets

Finding a set of basis functions Ψ that achieve such a transformation lies at the heart of
many lossy compression techniques. For instance, in image processing the JPEG 2000
standard is a widely used lossy compression technique that relies on this principle. The
original image x is transformed into w using the so-called Discrete Cosine Transform.
The basis matrix Ψ is orthogonal, so x and w have the same l2 norm. In the original
signal x the length is spread across many of its coefficients. On the other hand, most
of the length of w is concentrated in a few of its coefficients. A large fraction of the

3.3 Forming the basis matrix

5

Fig. 3.1 Original image x (left) and its Haar basis transformation w (right). See next
chapter for more details on Haar wavelets.
entries in w are very close to zero. By deleting these entries in w and only storing
the non-zero coefficients (and the corresponding basis functions), we can obtain a
ˆ This allows us to significantly reduce the amount of data that
compressed version w.
needs to be stored without affecting the visual quality in the reconstructed image
ˆ = Ψw.
ˆ
x
It is important to note here that the choice of basis functions Ψ typically has a
significant effect on the performance of the reconstruction algorithms.
The simplest wavelet basis transformation is based on the Haar wavelets. Figure
?? vizualises a basis transformation of the original image x to w. This example uses a
Haar wavelet basis at the first scale. We will explain the Haar wavelet transformations
of images and videos in more detail in the next chapter. Dark areas correspond to small
coefficients. Note that most entries in w are near zero. In practice, we approximate
these entries as zero and treat w as sparse.

3.2.2

Daubechies Wavelets

Intuition. Where do the coeffs come from. Matrices. Boundary conditions.

3.3

Forming the basis matrix

1D, 2D, 3D case. Different scales
For a deeper introduction into wavelets see [4]. For more information on wavelet
compression techniques, see [2].

Chapter 4
Sparse Bayesian Learning
Sparse Bayesian Learning [6] is a general Bayesian framework within supervised
Machine Learning. It can be applied to both regression and classification tasks. The
Relevance Vector Machine, or RVM, is a particular specialisation of the Sparse Bayesian
Learning model which has identical functional form to the Support Vector Machine
(SVM). However, the RVM comes with a number of key advantages over the SVM.
The solution produced by a RVM is typically much sparser than the solution by a
comparable SVM. Furthermore, the RVM is a probabilistic model and as such, allows
us to estimate error bounds in its predictions.
In this chapter, we will derive the Sparse Bayesian Learning model for regression. We
will summarise both the original inference algorithm [6] and also the faster “Sequential
Sparse Bayesian Learning Algorithm” [7].

4.1

Model Specification

We are given a data set of N input vectors {x(i) }N
i=1 and their associated targets
(i) N
{y }i=1 . The input vectors live in D-dimensional space, x ∈ RD . The targets are real
values, y ∈ R. 1
We model the data using a linearly-weighted sum of M fixed basis functions
{ϕj (·)}M
j=1 and base our predictions on the function f (·) defined as
M

wj ϕj (x) = wT ϕ(x)

f (x; w) =

(4.1)

j=1
1

When using the Sparse Bayesian model for regression, we assume the targets are real-valued. It is
also possible to use the model for classification in which cased the targets are assumed to be discrete
class labels.

7

4.1 Model Specification

where w = [w1 , . . . , wM ]T and ϕ(·) = [ϕ1 (·), . . . , ϕM (·)]T . Using a large number M of
non-linear basis functions ϕj : RD → R allows for a highly flexible model.
The Relevance Vector Machine, or RVM, is a specialisation of the Sparse Bayesian
Learning model in which the basis functions take the form of kernel functions
ϕj (·) ≡ K · , x(j) .
This defines a basis function for each training data point x(i) . Typically, we also
include an additional bias term ϕ0 (·) ≡ 1, so that M = N + 1. The RVM has identical
functional form to the popular Support Vector Machine (SVM), but superior properties.
It typically gives sparser solutions than the SVM and has the additional advantage of
providing confidence measures for its predictions.
However, in the following derivation, we will stick to the case of general basis
functions ϕj : RD → R. Thus M need not equal N + 1 and may, in fact, be a lot larger.
To train the model (4.1), i.e. find values for w that are optimal in some sense, we
make the standard assumption that our training data are samples from the model with
additive noise:
y (i) = f (x(i) ; w) + ϵ(i)
= wT ϕ(x(i) ) + ϵ(i)

i = 1, . . . , N.

(4.2)

The errors {ϵ(i) }N
i=1 are assumed to be independent samples from a zero-mean Gaussian
distribution with variance σ 2
p(ϵ(i) ) = N (ϵ(i) | 0, σ 2 )

i = 1, . . . , N.

(4.3)

Combining equation (4.2) with equation (4.1), we may express the model for the
complete data using matrix notation:
y = Φw + ϵ

(4.4)

T

where ϵ = ϵ(1) , · · · , ϵ(N ) . The N × M matrix Φ is known as the design matrix.
The ith row of Φ is given by ϕ(x(i) )T . The jth column of Φ is given by ϕj =
ϕj x(1) , · · · , ϕj x(N )

T

, which is also referred to as the jth basis vector. Thus

Φ = ϕ1 · · · ϕM









ϕ(x(1) )T


..


=

.
ϕ(x(N ) )T

8

4.1 Model Specification

Combining equation (4.4) and equation (4.3), we find that the complete data
likelihood function is given by
p y | w, σ 2 = N y | w, σ 2 IM
= (2πσ 2 )−N/2 exp −

1
||y − Φw||2
2
2σ

(4.5)

where IM is the M × M identity matrix.
So far, we have specified the general linear regression model. To get to the sparse
Bayesian formulation, we define a zero-mean Gaussian prior distribution over the
parameters w
M

N wj | αj−1

p(w | α) =
j=1

where α = [α1 , · · · , αM ]T is a vector of M hyperparameters. It is important to note
that each hyperparameter αj is solely responsible for controlling the strength of the
prior of its associated weight wj . If αj is large, the prior over wj is very strongly peaked
at zero. This form of the prior distribution is, more than anything, responsible for the
dramatic sparsity in the final model.
To complete the specification, we must define a prior over the noise parameter σ 2
and the a hyperprior over the hyperparameters α. Following the derivation in [6], we
use the following Gamma 2 priors
M

p(α | a, b) =

Gamma(αj | a, b)
j=1

p(β | c, d) = Gamma(β | c, d)
where β ≡ σ −2 .
As a side note, consider the prior of w after marginalising out the dependence on
the hyperpriors α. Since each wj is normally distributed with an unknown precision
parameter αj and since the (hyper)prior over αj is the Gamma distribution and
2

The Gamma distribution is defined by
Gamma(z | a, b) = Γ(a)−1 ba z a−1 exp(−bz)

where Γ(.) is the Gamma function defined by
∞

tz−1 exp(−t)dt.

Γ(z) =
0

z, a, b > 0

9

4.2 Model Inference

therefore conjugate to p(wj | αj ), it follows that the resulting integral can be evaluated
analytically
p(w | a, b) =

p(w | α)p(α | a, b)dα
M

N (wj | 0, αj−1 ) Gamma(αj | a, b)dαj

=
j=1
M

=

ba Γ(a + 12 )
1

j=1

(2π) 2 Γ(a)

wj2
b+
2

−(a+ 12 )

.

This corresponds to a product of independent Student-t density functions over the
weights wj . The choice a = b = 0 implies that p(w | a, b) ∝ M
j=1 1/|wj |. As discussed
in [6], it is this hierarchical formulation of the weight prior that is ultimately responsible
for encouraging sparse solutions.

4.2

Model Inference

We have specified the likelihood model for the data and a prior distribution over the
model parameters. The next step in Bayesian inference is to compute the posterior
distribution of the parameters. We begin by setting up Bayes’ Rule
p(w, α, σ 2 | y) =

p(y | w, α, σ 2 )p(w, α, σ 2 )
p(y | w, α, σ 2 )p(w, α, σ 2 )dwdαdσ 2

(4.6)

The integral in the denominator of (4.6) is computationally intractable and we must
resort to an alternative strategy. First, we decompose the left-hand-side of equation
(4.6) as
p(w, α, σ 2 | y) = p(w | y, α, σ 2 )p(α, σ 2 | y).
Next, we use Bayes’ Rule to compute the posterior distribution of the weights given α
and σ 2
p(y | w, σ 2 )p(w | α)
p(w | y, α, σ 2 ) =
(4.7)
p(y | α, σ 2 )
The denominator of the right-hand-side is known as the marginal likelihood and
given by
p(y | α, σ 2 ) = p(y | w, σ 2 )p(w | α)dw
(4.8)
Since α and σ 2 are treated as fixed quantities in equation (4.7), the Gaussian density
p(w | α) is the conjugate prior to the Gaussian likelihood function p(y | w, σ 2 ). Thus,

10

4.2 Model Inference

the integral in equation (4.8) is a convolution of two Gaussians and therefore equal to
another Gaussian:
p(y | α, σ 2 ) = N (y | 0, C)
1
= (2π)−N/2 |C|−1/2 exp − y T C −1 y
2
where
C = σ 2 IN + ΦA−1 ΦT .

(4.9)

The posterior distribution for w is a also a Gaussian:
p(w | y, α, σ 2 ) = N (w | µ, Σ).

(4.10)

Its mean µ and covariance matrix Σ are given by
Σ = σ −2 ΦT Φ + A

−1

µ = σ −2 ΣΦT y

(4.11)

(4.12)

with A = diag(α).
Finally, we need to find the posterior of the hyperparameters, p(α, σ 2 | y). This
part is computationally intractable, so instead we approximate the posterior by a
delta-function at its mode. Hence, the problem reduces to finding the values of α and
σ 2 that maximise p(α, σ 2 | y) ∝ p(y | α, σ 2 )p(α)p(σ 2 ).
Here, we make the simplifying assumption that a = b = c = d = 0, giving us uniform
(but improper) hyperpriors (see [6] for the general case). Maximising p(α, σ 2 | y) is
then equivalent to maximising the marginal likelihood, or equivalently, its logarithm
L(α, σ 2 ) = log p(y | α, σ 2 ) = log N (y | 0, C)
1
= − N log 2π + log |C| + y T C −1 y
2

(4.13)

The procedure of finding α and σ 2 that maximise the (log) marginal likelihood (4.13)
is also known as type-II Maximum likelihood and evidence approximation.

11

4.2 Model Inference
Algorithm 1 Sparse Bayesian Learning: Original Training Algorithm
Choose some initial positive values for σ 2 and αj for j = 1, · · · , M
repeat
A = diag(α)
−1
4:
Σ = σ −2 ΦT Φ + A
5:
µ = σ −2 ΣΦT y

1:
2:
3:

6:
7:
8:
9:
10:
11:

for j = 1, · · · , M do
γj = 1 − αj Σjj
αj = γj /µ2j
end for
σ 2 = ||y − Φµ||2 /(N −
until Convergence

4.2.1

j

γj )

Original Training Algorithm

The original training algorithm in [6] is derived by setting the derivatives of (4.13) to
zero. We obtain the following update equations for α and σ 2 :
αjnew =

γj
µ2j

||y − Φµ||2
(σ 2 )new =
N − j γj

(4.14)

(4.15)

where µj is the jth component of the posterior mean µ (4.12). The quantities γj are
defined by
γj = 1 − αj Σjj
where Σjj is the jth diagonal element of the posterior covariance Σ (4.11).
To train the model, we can start by giving α and σ 2 some initial values and evaluate
the mean and covariance of the weights posterior using equations (4.12) and (4.11),
respectively. Next we alternate between re-estimating the hyperparamaters α and σ 2
using (4.14) and (4.15) and updating the posterior mean and covariance parameters
using (4.12) and (4.11). We continue until a relevant convergence criterion is met.
For example, we may choose to stop if the change in the marginal likelihood - or,
alternatively, the change in the parameter values - between two iterations is below a
certain pre-defined threshold.
This procedure is summarised in Algorithm 1.

12

4.2 Model Inference

During training, it is typically observed that many of the hyperparameters αj tend
to infinity. Equations (4.12) and (4.11) imply that the weights wj corresponding to
these hyperparameters have a posterior distribution where the mean and the variance
are both zero, meaning their posterior is infinitely peaked at zero. As a consequence,
the corresponding basis functions ϕj (·) are effectively removed from the model and we
achieve sparsity.
In the case of the RVM, where ϕj (·) ≡ K(·, x(j) ), the input vectors x(j) corresponding to the remaining non-zero weights are known as the relevance vectors of the
model.

4.2.2

Sequential Sparse Bayesian Learning Algorithm

A central drawback of the training algorithm discussed in the previous section is its
speed. The computational complexity scales with the cube of the number of basis
functions. During training, as basis functions are pruned from the model, the algorithm
accelerates. Nevertheless, if M is very large, the procedure can be very expensive to
run.
An alternative strategy of maximising the marginal likelihood (4.13) was developed
by [7], resulting in a highly accelerated training algorithm: the Sequential Sparse
Bayesian Learning Algorithm. It starts with a single basis function and maximises
the marginal likelihood by sequentially adding and deleting candidate basis functions.
This significantly reduces the computational complexity of the algorithm.
To derive the algorithm, we follow the analysis in [5] and consider the dependence of
the marginal likelihood L(α, σ 2 ) on a single hyperparameter αj . First, we decompose
the matrix C, defined in (4.9), as follows:
−1
αm
ϕm ϕTm + αj−1 ϕj ϕTj

C = σ 2 IN +
m̸=j

= C−j + αj−1 ϕj ϕTj
−1
where C−j ≡ σ 2 IN + m̸=j αm
ϕm ϕTm is C without the contribution of the jth basis
vector ϕj . Making use of standard identities [WHICH ONES?] for matrix inverses and
determinants, we can express |C| and C −1 as

C

−1

=

−1
C−j

−1
−1
C−j
ϕj ϕTj C−j
−
−1
αj + ϕTj C−j
ϕj

−1
ϕj
|C| = |C−j | 1 + αj−1 ϕTj C−j

13

4.2 Model Inference
ℓ(αj , σ 2 )

ℓ(αj , σ 2 )

qj2 > sj

10−7

s2j
2
qj −sj

qj2 < sj

100

107

αj

10−7

100

107

αj

Fig. 4.1 Example plots of ℓ(αj , σ 2 ) against αj illustrating the stationary points when
qj2 > sj (left) and qj2 < sj (based on [5]).
This allows us to decompose the marginal likelihood:
L(α, σ 2 ) = L(α−j , σ 2 ) +
2

qj2
1
log αj − log(αj + sj ) +
2
αj + sj

(4.16)

2

≡ L(α−j , σ ) + ℓ(αj , σ )
This conveniently seperates terms in αj in ℓ(αj , σ 2 ) from the remaining terms in
L(α−j , σ 2 ), which is the (log) marginal likelihood with the basis vector ϕj excluded.
The quantity sj is the sparsity factor, defined as
−1
sj = ϕTj C−j
ϕj .

It serves as a measure of how much the marginal likelihood would decrease if we added
ϕj to the model. The quantity qj , on the other hand, is known as the quality factor. It
is defined as
−1
qj = ϕTj C−j
y
and measures the extent to which ϕj increases L(α, σ 2 ) by helping to explain the data
y. Thus, a particular basis vector ϕj should not be included in the model if its sparsity
factor sj is large, unless it is offset by a large quality factor qj .
We can see this more explicitly if we consider the first derivative of ℓ(αj , σ 2 ) with
respect to αj [5]
αj−1 s2j − (qj2 − sj )
∂ℓ(αj , σ 2 )
=
∂αj
2(αj + sj )2

14

4.2 Model Inference

Equating it to zero (and noting that αj is an inverse-variance and therefore positive),
we obtain the following solution for αj :
αj =




s2j /(qj2 − sj ) if qj2 > sj
.
 +∞
otherwise

(4.17)

The solution (4.17) is illustrated in Figure 4.1.
It follows that, if, during training, a candidate basis vector ϕj is currently included
in the model (meaning αj < ∞) even though qj2 ≤ sj , then αj should be set to ∞ and
ϕj should be pruned from the model. On the other hand, if ϕj is currently excluded
from the model (i.e. αj = ∞), but qj2 > sj , then αj should be set to s2j /(qj2 − sj ) and
ϕj should be added to the model. Furthermore, if ϕj is included and qj2 > sj , then we
may also re-estimate αj . Each step in the algorithm (weakly) increases the marginal
likelihood. Thus we are guaranteed to find a maximum.
During the algorithm, we must maintain and update values of the quality factors and
sparsity factors for all basis functions, as well as the posterior mean µ and covariance Σ
of the weights w. In practice, it easier to keep track of the quantities Qm = ϕTm C −1 ϕm
and Sm = ϕTm C −1 y which can also be written as (using the Woodbury Identity)
Sm = σ −2 ϕTm ϕm − σ −4 ϕTm ΦΣΦT ϕm

(4.18)

Qm = σ −2 ϕTm y − σ −4 ϕTm ΦΣΦT y

(4.19)

where Σ and Φ contain only the basis functions that are currently included in the
model.
The factors sm and qm can by obtained from Sm and Qm as follows:
sm =

αm Sm
αm − Sm

(4.20)

qm =

αm Q m
αm − Sm

(4.21)

Note that if αm = ∞, then qm = Qm and sm = Sm .
We have summarized the procedure in Algorithm 2. After initializing the standard
deviation σ 2 in step 1, we add the first basis function ϕj to the model. We could
initialize with any basis vector, but in step 2, we pick the one with the largest normalized
projection on the target vector y, i.e. we choose j = arg maxm ||ϕTm y||2 /||ϕm ||2 . In

15

4.3 Making Predictions
Algorithm 2 Sequential Sparse Bayesian Learning Algorithm [7]
1:
2:

Initialise σ 2 .
Add basis function ϕj to the model, where j = arg maxm ||ϕTm y||2 /||ϕm ||2 .
Set αj =

||ϕj ||2
T
||ϕj y||2 /||ϕj ||2 −σ 2

. Set αm = ∞ for m ̸= j.
−1

3:
4:
5:
6:
7:
8:
9:
10:
11:

and µ = σ −2 ΣΦT y which are scalars initially.
Compute Σ = σ −2 ΦT Φ + A
Compute Sm , Qm , sm and qm for m = 1, · · · , M using (4.18) − (4.21).
repeat
Select some candidate basis vector ϕj .
if qj2 > sj and αj = ∞ then add ϕj to the model and update αj .
if qj2 > sj and αj < ∞ then re-estimate αj .
if qj2 < sj and αj < ∞ then delete ϕj from the model and set αj = ∞.
Update σ 2 = ||y − Φw||/(N − M + m αm Σmm )[6].
Update Σ, µ and Sm , Qm , sm , qm for m = 1, · · · , M .
until Convergence

step 3 we compute the model statistics and in step 4 we begin the large loop of the
algorithm. There are two things to note here. First, in step 5, we need to select a
candidate basis vector ϕj . We are free to pick one at random. Alternatively, it is
possible to compute the change in the marginal likelihood for each ccandidate basis
vector and choose the one that would give us the largest increase. Second, we would
usually like to estimate the noise variance σ 2 from the data, as is done in step 9.
However, in practice, we may decide to set σ 2 in advance in step 1 and keep it fixed
throughout the algorithm. If we decide to do so, then we can perform the updates in
step 10 using very efficient update formulae that do not require matrix inversions. The
formulae can be found in the appendix of [7]. If we do decide to update σ 2 in step 9,
then we must use the full equations (4.11), (4.12) and (4.18)-(4.21).

4.3

Making Predictions

Once we have trained the model, we may use it to predict the target y ∗ for a new input
vector x∗ . To do so, we would like to compute the predictive distribution
p(y ∗ | y) =

p(y ∗ | w, α, σ 2 )p(w, α, σ 2 | y) dw dα dσ 2 .

We cannot compute this integral analytically, nor do we actually know the posterior
of all the model parameters. Instead, we use the type-II maximum likelihood solutions
for α and σ 2 that we obtained during training and base our predictions on the posterior

16

4.3 Making Predictions

distribution of the weights conditioned on α and σ 2 . The predictive distribution for
x∗ is then:
p(y ∗ | y, α, σ 2 ) = p(y ∗ | w, σ 2 )p(w | y, α, σ 2 ) dw
(4.22)
Both factors in the integrand are Gaussians, and we can therefore readily compute
the integral to get
p(y ∗ | y, α, σ 2 ) = N (y ∗ | µ∗ , (σ 2 )∗ )
(4.23)
The predictive mean is given by
µ∗ = µT ϕ(x∗ )

(4.24)

and the predictive variance is given by
(σ 2 )∗ = σ 2 + ϕ(x∗ )T Σϕ(x∗ )

(4.25)

Equation (4.24) implies that, if we want to produce point predictions, we may
simply set the weights w equal to posterion mean µ which is typically very sparse.
If we are also interested in error bars for our predictions, we can obtain them using
Equation (4.25). The error bars consist of two parts, the noise in the data σ 2 and the
uncertainty in the weights.
For more details and derivations on Sparse Bayesian Learning, see [5–7].

Chapter 5
Design of the Multi-Scale Cascade
of Estimations Algorithm
Bringing all building blocks together. Description and explanation of the algorithm
So far, we have not addressed the central question: How do we solve the compressive
sensing problem (2.2)? Various deterministic approaches have been developed in recent
years. See [3] for an overview.
In the MPhil project, we will employ a probabilistic technique based on Sparse
Bayesian Learning. In particular, we will use the Relevance Vector Machine (RVM) [6, 7]
to reconstruct w from the measurements y. Following that, we obtain a reconstructed
version of the desired signal x by pre-multiplying w by Ψ to obtain the desired signal.

5.1

Interpolator

We use a sensing matrix Ω that acts as signal mask. That is, we obtain the N × M
matrix Ω by taking the M × M identity matrix IM and deleting (M − N ) rows. This
corresponds to a subsampled signal in which we only measured N pixel values. For
this specific class of sensing matrices, the problem of reconstructing the original signal
is also known as interpolation.
In order to reconstruct the image, we use the estimated posterior mean to “predict”
what a pixel value y ∗ should be at a location x∗ in which information was missing:
y ∗ = wT ψ(x∗ )

(5.1)

5.1 Interpolator

18

ˆ (right) using a cascade of
Fig. 5.1 Corrupted signal y (left) and reconstructed signal x
3 RVMs with Haar basis functions (see [3]).
Apart from achieving sparse solutions, one further desirable feature of the RVM is
that the model provides error bars for its predictions. This is used in [3] to construct a
multi-scale cascade of RVM estimations and achieve significant performance boosts.
An example of this can be seen in Figure 5.1.

Chapter 6
Implementation Details and Code
optimization
This chapter gives a brief description of the current state of our implementation of the
3D signal reconstructer.

6.1

Haar basis functions

The RVM takes as input a target vector (y) and a basis matrix (Ψ). In this respect,
it is agnostic about whether the signal is an image or video or of some other type
alltogether. Most of this information is encoded in the basis matrix Ψ. It is therefore
important, and often challenging, to select a good set of basis functions.
Our current implementation uses 3-dimensional Haar wavelet basis functions. I
will show how the basis matrix Ψ is constructed by briefly describing how the discrete
Haar wavelet transform is performed on 1D, 2D and finally on 3D signals.

6.1.1

1D Haar wavelet transform

Consider a 1-dimensional signal s = {s1 , . . . , sr } ∈ Rr (r for “rows”), where, for
simplicity, we assume that r is a power of 2. The Haar wavelet transform can be
performed at various resolution scales. The transform at the first scale is given by:
1
s = {s1 , . . . , sr } → √ {s1 + s2 , s3 + s4 , . . . , sr−1 + sr , s1 − s2 , . . . , sr−1 − sr } = sˆ(1)
2
The first half of the signal is replaced by scaled averages of adjacent elements and the
second half is replaced by scaled differences of adjacent elements. By performing this

20

6.1 Haar basis functions

transform again on the first half of sˆ(1) while keeping the second half fixed, we get the
Haar wavelet transform at the second scale sˆ(2) . To get the third scale transform sˆ(3) ,
we perform the initial transform on the first quarter of sˆ(2) while keeping the rest of the
signal fixed. We may continue this process until we reach the ith scale, where 2i = r.
From here on, we will only consider the first scale transform sˆ(1) and we will omit
the (1) superscript. We can express the transform as a multiplication by an orthogonal
r × r matrix W given by
 
Φr
(6.1)
W = 
Ψr
where Φr and Ψr are (r/2) × r matrices1 given by


1


1 0
Φr = √ 
.
2  ..

1
0
..
.

0
1
..
.

0 ···
1 ···
.. . .
.
.



0
0
..
.

0 0 0 0 ··· 1 1

and





0

0

.. 

.




1 −1 0 0 · · · 0 0



0 0 1 −1 · · · 0 0 

1 
Ψr = √ 
.. .. .. . . .. .. 

 ..
2 . . . .
. . . 


0 0 0 0 · · · 1 −1
In the signal processing literature, Φr is referred to as a low pass filter, while Ψr is
referred to as a high pass filter. Φr outputs an average of the signal and Ψr outputs
the details of the signal.

6.1.2

2D Haar wavelet transform

Let A ∈ Rr×c be a 2-dimensional signal (e.g. an image). For simplicity, we will assume
that both r and c are powers of 2 (though not necessarily equal).
It is simple to obtain A’s Haar wavelet transform Aˆ at the first scale. This is done
by first applying the 1-dimensional transform individually to each column of A to
obtain a temporary matrix Aˆtemp . Next, we apply the 1-dimensional haar wavelet
ˆ
transform individually to each row of Aˆtemp to obtain A.
1

Note that the matrix Ψr used here is different to the matrix Ψ that was used in the previous
chapter (which corresponds to W T here).

21

6.1 Haar basis functions
We can again express the transform as a multiplication of matrices:




Φr
Aˆ =   A ΦTc ΨTc
Ψr

(6.2)

where Φr and Ψr are as before and Φc and Ψc are of similar form but each have
dimensions (c/2) × c. This is the transform that was used to generate the RHS
of Figure ??. We note that the high-pass filters essentially detect edges of various
orientations in the image.
However, as it currently stands, we cannot use this form of the basis transformation
for the reconstruction algorithm. Recall that the RVM requires a vector of measurements
as opposed to a matrix and also that it requires a single basis matrix, not a basis
transform as given in (6.2).
To do this, we store the 2-dimensional signal A as a long column vector a of length
rc by pasting the individual columns of A one after another. The basis transformation
of a can then be expressed as
ˆ = Wa
a
where W is a rc × rc matrix given by




Φc ⊗ Φr


Φ ⊗ Ψ 
r
 c
W =

 Ψc ⊗ Φr 


Ψc ⊗ Ψr
The symbol ⊗ denotes the Kronecker product. The kronecker product P ⊗ Q
between matrices P and Q with dimensions mP × nP and mQ × nQ , respectively, is
defined to be the block matrix









p1,1 Q
p2,1 Q
..
.

p1,2 Q
p2,2 Q
..
.

···
···
..
.

p1,nP Q
p2,nP Q
..
.

pmP ,1 Q pmP ,2 Q · · · pmP ,nP Q

of size mP mQ × nP nQ .










6.2 Update formulae, details on RVM implementation

6.1.3

22

3D Haar wavelet transform

Let V ∈ Rr×c×s be a 3-dimensional signal such as a video. V has r rows, c columns
and s slices, and we assume that r, c and s are all powers of 2. We may visualize V as
a “volume” with 2 spacial dimensions and one time dimension corresponding to frames
of the video.
To obtain the Haar wavelet transform Vˆ of V , we first perform the 1-dimensional
transform individually on each column in every slice of V to get Vˆtemp1 . We then
perform the 1D transform on every row in every slice of Vˆtemp1 to get Vˆtemp2 . Finally,
we perform the 1D transform across the slices for every row and column to get Vˆ .
However, like in the 2-dimensional case, we need to be able to pass a single vector
of coefficients and a single basis matrix to the RVM. To do this, we vectorize V as
follows. First, we vectorize each individual slice of V as before in the 2D case. Then,
we stack all these vectors on top each other to get one very long column vector v of
length rcs. The Haar wavelet transform is given by
vˆ = W v
where





Φ ⊗ Φc ⊗ Φr
 s

Φ ⊗ Φ ⊗ Ψ 
 s
c
r


Φ ⊗ Ψ ⊗ Φ 
 s
c
r


Φ ⊗ Ψ ⊗ Ψ 
c
r
 s

W =
 Ψs ⊗ Φc ⊗ Φr 




 Ψs ⊗ Φc ⊗ Ψr 




 Ψs ⊗ Ψc ⊗ Φr 


Ψs ⊗ Ψc ⊗ Ψr
Comparing notation to the previous chapter, what we refer to as W here is the transpose
of what was previously denoted as Ψ. And since v = W T vˆ, we see that v corresponds
to what was previously called x.

6.2

Update formulae, details on RVM implementation

Chapter 7
Results
We have obtained some results with our current implementation. The implementation
uses the Haar wavelet transform at the first scale.
Our example video has a resolution of 128 by 128 pixels and consists of a total of
64 frames. Thus, r = 128, c = 128 and s = 64. Note that even for such a relatively
small sample, the size of the basis matrix Ψ is (128 ∗ 128 ∗ 64) × (128 ∗ 128 ∗ 64) =
1048576 × 1048576. Even in single precision, storing this matrix would require around
4 terrabytes.
For this reason, we have split the original input signal into 8 × 8 × 8 blocks and
perform the algorithm on the individual blocks.
In Figures 3.1 and 3.2, we have included a sample frame from the corrupted test
video and the same frame after reconstruction.
In Figure 3.1, we corrupted the video by deleting 30% of the pixel values in the
first frame and deleting the same pixel values in each subsequent frame (so the same
pixels are missing in each frame). Figure 3.2 uses the same corruption scheme but we
deleted 50% rather than 30% of pixel values.
These initial results are promising, though clearly there are still improvements to
be made.

24

Fig. 7.1 Sample frame from corrupted video (left) and the reconstructed video (right)

Fig. 7.2 Sample frame from corrupted video (left) and the reconstructed video (right)

Chapter 8
Conclusion

References
[1] Candès, E. J. and Wakin, M. B. (2008). An introduction to compressive sampling.
Signal Processing Magazine, IEEE, 25(2):21–30.
[2] DeVore, R. A., Jawerth, B., and Lucier, B. J. (1992). Image compression through
wavelet transform coding. Information Theory, IEEE Transactions on, 38(2):719–
746.
[3] Pilikos, G. (2014). Signal reconstruction using compressive sensing. MPhil thesis,
University of Cambridge.
[4] Stollnitz, E. J., DeRose, T. D., and Salesin, D. H. (1995). Wavelets for computer
graphics: a primer. 1. Computer Graphics and Applications, IEEE, 15(3):76–84.
[5] Tipping, A. and Faul, A. (2002). Analysis of sparse bayesian learning. Advances in
neural information processing systems, 14:383–389.
[6] Tipping, M. E. (2001). Sparse bayesian learning and the relevance vector machine.
The journal of machine learning research, 1:211–244.
[7] Tipping, M. E., Faul, A. C., et al. (2003). Fast marginal likelihood maximisation
for sparse bayesian models. In AISTATS.

