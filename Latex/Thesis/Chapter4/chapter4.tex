%%%%%%%%% RVM Nomenclature %%%%%%%%%
\nomenclature[a-rvm_M]{$N$}{Number of training examples}
\nomenclature[a-rvm_x]{$\bm x^{(i)}$}{$i$th input vector}
\nomenclature[a-rvm_y]{$y^{(i)}$}{$i$th target}
\nomenclature[a-rvm_w]{$\bm w$}{RVM weights vector}
\nomenclature[z-rvm]{RVM}{Relevance Vector Machine}

%%%%%%%%%%% RVM chapter %%%%%%%%%%%%
\chapter{Sparse Bayesian Learning}
\emph{Sparse Bayesian Learning} \cite{tipping2001} is a general Bayesian framework within supervised Machine Learning. 
It can be applied to both regression and classification tasks.
The \emph{Relevance Vector Machine}, or \emph{RVM}, is a particular specialisation of the Sparse Bayesian Learning model which has identical functional form to the Support Vector Machine (SVM).
However, the RVM comes with a number of key advantages over the SVM. 
The solution produced by a RVM is typically much sparser than the solution by a comparable SVM.
Furthermore, the RVM is a probabilistic model and as such, allows us to estimate error bounds in its predictions.

In this chapter, we will derive the Sparse Bayesian Learning model for regression.
We will summarise both the original inference algorithm \cite{tipping2001} and also the faster ``Sequential Sparse Bayesian Learning Algorithm'' \cite{tipping2003}.

\section{Model Specification}
We are given a data set of $N$ input vectors $\{\bm x^{(i)}\}^N_{i=1}$ and their associated \emph{targets} $\{y^{(i)}\}_{i=1}^N$.
The input vectors live in $D$-dimensional space, $\bm x \in \mathbb{R}^D$.
The targets are real values, $y \in \mathbb{R}$.
\footnote{When using the Sparse Bayesian model for regression, we assume the targets are real-valued.
  It is also possible to use the model for classification in which cased the targets are assumed to be discrete class labels.
}

We model the data using a linearly-weighted sum of $M$ fixed basis functions $\{\phi_j(\cdot)\}_{j=1}^M$ and base our predictions on the function $f(\cdot)$ defined as
\begin{equation} 
  \label{rvm:function}
  f(\bm x; \bm w) = \sum_{j=1}^M w_j \phi_j(\bm x) = \bm w^T \bm \phi(\bm x)
\end{equation}
where $\bm w = [w_1, \dots, w_M]^T$ and $\bm \phi(\cdot) = [\phi_1(\cdot), \dots, \phi_M(\cdot)]^T$.
Using a large number $M$ of non-linear basis functions $\phi_j : \mathbb{R}^D \to \mathbb{R}$ allows for a highly flexible model. 

The \emph{Relevance Vector Machine}, or RVM, is a specialisation of the Sparse Bayesian Learning model in which the basis functions take the form of \emph{kernel functions} 
\begin{equation*}
  \phi_j(\cdot) \equiv K\left(\cdot\,,\,\bm x^{(j)}\right).
\end{equation*}
This defines a basis function for each training data point $\bm x^{(i)}$.
Typically, we also include an additional \emph{bias} term $\phi_0(\cdot) \equiv 1$, so that $M = N+1$.
The RVM has identical functional form to the popular Support Vector Machine (SVM), but superior properties.
It typically gives sparser solutions than the SVM and has the additional advantage of providing confidence measures for its predictions.

However, in the following derivation, we will stick to the case of general basis functions $\phi_j:\mathbb{R}^D\to\mathbb{R}$.
Thus $M$ need not equal $N+1$ and may, in fact, be a lot larger.

To train the model (\ref{rvm:function}), i.e. find values for $\bm w$ that are optimal in some sense, we make the standard assumption that our training data are samples from the model with additive noise:
\begin{equation}
  \label{rvm:lklhd_1}
  \begin{split}
    y^{(i)} &= f(\bm x^{(i)}; \bm w) + \epsilon^{(i)}\\
    &= \bm w^T \bm \phi(\bm x^{(i)}) + \epsilon^{(i)}\qquad i = 1, \dots, N.
  \end{split}
\end{equation}
The errors $\{\epsilon^{(i)}\}_{i=1}^N$ are assumed to be independent samples from a zero-mean Gaussian distribution with variance $\sigma^2$
\begin{equation}
  \label{rvm:error}
  p(\epsilon^{(i)}) = \mathcal{N}(\epsilon^{(i)}\,|\,0, \sigma^2)\qquad i = 1, \dots, N.
\end{equation}

Combining equation (\ref{rvm:lklhd_1}) with equation (\ref{rvm:function}), we may express the model for the complete data using matrix notation:
\begin{equation}
  \label{rvm:lklhd_2}
  \bm y = \bm\Phi \bm w + \bm \epsilon
\end{equation}
where $\bm \epsilon = \left[\epsilon^{(1)}, \cdots, \epsilon^{(N)}\right]^T$.
The $N\times M$ matrix $\bm \Phi$ is known as the design matrix. 
The $i$th row of $\bm \Phi$ is given by $\bm \phi(\bm x^{(i)})^T$.
The $j$th column of $\bm \Phi$ is given by $\bm \phi_j = \left[\phi_j\left(\bm x^{(1)}\right), \cdots, \phi_j\left(\bm x^{(N)}\right)\right]^T$, which is also referred to as the $j$th \emph{basis vector}.
Thus
\begin{equation*}
  \bm\Phi =
  \begin{bmatrix}
    \bm\phi_1 & \cdots & \bm\phi_M
  \end{bmatrix}
  =
  \begin{bmatrix}
    \bm\phi(\bm x^{(1)})^T\\
    \vdots\\
    \bm\phi(\bm x^{(N)})^T
  \end{bmatrix}
\end{equation*}

Combining equation (\ref{rvm:lklhd_2}) and equation (\ref{rvm:error}), we find that the complete data likelihood function is given by
\begin{equation}
  \label{rvm:complete_lk}
  \begin{split}
    p\left(\bm y\,|\,\bm w,\sigma^2\right) &= \mathcal{N}\left(\bm y\,|\,\bm w,\sigma^2 \bm I_M\right)\\
    &= (2\pi\sigma^2)^{-N/2} \exp\left\{-\frac{1}{2\sigma^2}||\bm y - \bm\Phi\bm w||^2\right\}
  \end{split}
\end{equation}
where $\bm I_M$ is the $M\times M$ identity matrix.

So far, we have specified the general linear regression model.
To get to the sparse Bayesian formulation, we define a zero-mean Gaussian prior distribution over the parameters $\bm w$
\begin{equation}
  \label{rvm:prior}
  p(\bm w\,|\,\bm \alpha) = \prod_{j=1}^M \mathcal{N}\left(w_j\,|\,\alpha_j^{-1}\right)
\end{equation}
where $\bm \alpha = \left[\alpha_1,\cdots,\alpha_M\right]^T$ is a vector of $M$ hyperparameters.
It is important to note that each hyperparameter $\alpha_j$ is solely responsible for controlling the strength of the prior of its associated weight $w_j$.
If $\alpha_j$ is large, the prior over $w_j$ is very strongly peaked at zero.
This form of the prior distribution is, more than anything, responsible for the dramatic sparsity in the final model.

To complete the specification, we must define a prior over the noise parameter $\sigma^2$ and the a hyperprior over the hyperparameters $\bm \alpha$.
Following the derivation in \cite{tipping2001}, we use the following Gamma
\footnote{
The Gamma distribution is defined by   
\[
\GammaDist(z\,|\,a,b) = \Gamma(a)^{-1} b^a z^{a-1} \exp(-bz)  \qquad z,a,b > 0
\]
where $\Gamma(.)$ is the Gamma function defined by 
\[
\Gamma(z) = \int_0^\infty t^{z-1} \exp(-t) dt.
\]
} priors
\begin{equation*}
  p(\bm\alpha\,|\,a,b) = \prod_{j=1}^M \GammaDist(\alpha_j\,|\,a,b)
\end{equation*}
\begin{equation*}
  p(\beta\,|\,c,d) = \GammaDist(\beta\,|\,c,d)
\end{equation*}
where $\beta \equiv \sigma^{-2}$.

As a side note, consider the prior of $\bm w$ after marginalising out the dependence on the hyperpriors $\bm\alpha$.
Since each $w_j$ is normally distributed with an unknown precision parameter $\alpha_j$ and since the (hyper)prior over $\alpha_j$ is the Gamma distribution and therefore conjugate to $p(w_j\,|\,\alpha_j)$, it follows that the resulting integral can be evaluated analytically
\begin{equation*}
  \begin{split}
    p(\bm w\,|\,a,b) &= \int p(\bm w\,|\,\bm\alpha) p(\bm\alpha\,|\,a,b)d\bm\alpha\\
    &= \prod_{j=1}^M \int \mathcal{N}(w_j\,|\,0,\alpha_j^{-1}) \GammaDist(\alpha_j|\,a,b)d\alpha_j\\
    &= \prod_{j=1}^M \frac{b^a\Gamma(a+\frac{1}{2})}{(2\pi)^{\frac{1}{2}}\Gamma(a)}\left(b+\frac{w_j^2}{2}\right)^{-(a+\frac{1}{2})}.
\end{split}
\end{equation*}
This corresponds to a product of independent Student-t density functions over the weights $w_j$.
The choice $a=b=0$ implies that $p(\bm w \,|\,a,b) \propto \prod_{j=1}^M 1/|w_j|$.
As discussed in \cite{tipping2001}, it is this hierarchical formulation of the weight prior that is ultimately responsible for encouraging sparse solutions.


\section{Model Inference}
We have specified the likelihood model for the data and a prior distribution over the model parameters.
The next step in Bayesian inference is to compute the posterior distribution of the parameters.
We begin by setting up Bayes' Rule
\begin{equation}
\label{rvm:bayes1}
  p(\bm w, \bm \alpha, \sigma^2 \,|\,\bm y) = \frac{p(\bm y \,|\, \bm w, \bm \alpha, \sigma^2) p(\bm w, \bm \alpha, \sigma^2)}
  {\int p(\bm y \,|\, \bm w, \bm \alpha, \sigma^2) p(\bm w, \bm \alpha, \sigma^2)d\bm w d\bm \alpha d\sigma^2}
\end{equation}

The integral in the denominator of (\ref{rvm:bayes1}) is computationally intractable and we must resort to an alternative strategy.
First, we decompose the left-hand-side of equation (\ref{rvm:bayes1}) as
\begin{equation}
  \label{rvm:decompose}
  p(\bm w, \bm \alpha, \sigma^2\,|\,\bm y) = p(\bm w\,|\,\bm y, \bm \alpha, \sigma^2) p(\bm\alpha,\sigma^2\,|\,\bm y).
\end{equation}
Next, we use Bayes' Rule to compute the posterior distribution of the weights given $\bm\alpha$ and $\sigma^2$
\begin{equation}
  \label{rvm:bayes2}
  p(\bm w\,|\,\bm y, \bm \alpha, \sigma^2) = \frac{p(\bm y\,|\,\bm w,\sigma^2) p(\bm w\,|\,\bm \alpha)}{p(\bm y\,|\,\bm\alpha,\sigma^2)}
\end{equation}

The denominator of the right-hand-side is known as the \emph{marginal likelihood} and given by
\begin{equation}
  \label{rvm:ML1}
  p(\bm y\,|\,\bm\alpha,\sigma^2) = \int p(\bm y\,|\,\bm w,\sigma^2) p(\bm w\,|\,\bm \alpha) d\bm w
\end{equation}
Since $\bm\alpha$ and $\sigma^2$ are treated as fixed quantities in equation (\ref{rvm:bayes2}), the Gaussian density $p(\bm w\,|\,\bm\alpha)$ is the conjugate prior to the Gaussian likelihood function $p(\bm y\,|\,\bm w,\sigma^2)$.
Thus, the integral in equation (\ref{rvm:ML1}) is a convolution of two Gaussians and therefore equal to another Gaussian:
\begin{equation}
  \label{rvm:ML2}
  \begin{split}
    p(\bm y\,|\,\bm\alpha,\sigma^2) &= \mathcal{N}(\bm y\,|\,\bm 0,\bm C)\\
    &= (2\pi)^{-N/2} |\bm C|^{-1/2} \exp\left\{-\frac{1}{2}\bm y^T \bm C^{-1} \bm y\right\}
  \end{split}
\end{equation}
where
\begin{equation}
  \label{rvm:C}
  \bm C = \sigma^2 \bm I_N + \bm\Phi\bm A^{-1}\bm\Phi^T.
\end{equation}

The posterior distribution for $\bm w$ is a also a Gaussian, $p(\bm w\,|\,\bm y,\bm\alpha,\sigma^2) = \mathcal{N}(\bm w\,|\,\bm\mu,\bm\Sigma)$.
Its mean $\bm \mu$ and covariance matrix $\bm\Sigma$ are given by
\begin{equation}
  \label{rvm:Sigma}
  \bm\Sigma = \left(\sigma^{-2}\bm\Phi^T\bm\Phi + \bm A\right)^{-1}
\end{equation}
\begin{equation}
  \label{rvm:mu}
  \bm\mu = \sigma^{-2}\bm\Sigma\bm\Phi^T\bm y
\end{equation}
with $\bm A = \diag(\bm\alpha)$.

Finally, we need to find the posterior of the hyperparameters, $p(\bm\alpha,\sigma^2\,|\,\bm y)$.
This part is computationally intractable, so instead we approximate the posterior by a delta-function at its mode.
Hence, the problem reduces to finding the values of $\bm \alpha$ and $\sigma^2$ that maximise $p(\bm\alpha,\sigma^2\,|\,\bm y) \propto p(\bm y\,|\,\bm\alpha,\sigma^2)p(\bm\alpha)p(\sigma^2)$.

Here, we make the simplifying assumption that $a=b=c=d=0$, giving us uniform (but improper) hyperpriors (see \cite{tipping2001} for the general case).
Maximising $p(\bm\alpha,\sigma^2\,|\,\bm y)$ is then equivalent to maximising the marginal likelihood, or equivalently, its logarithm
\begin{equation}
  \label{rvm:ML}
  \begin{split}
    \mathcal{L}(\bm\alpha,\sigma^2) &= \log p(\bm y\,|\,\bm\alpha,\sigma^2) = \log \mathcal{N}(\bm y \,|\,\bm 0, \bm C)\\
    &=-\frac{1}{2}\left[N\log 2\pi + \log|\bm C| + \bm y^T\bm C^{-1} \bm y\right]
\end{split}    
\end{equation}
The procedure of finding $\bm\alpha$ and $\sigma^2$ that maximise the (log) marginal likelihood (\ref{rvm:ML}) is also known as \emph{type-II Maximum likelihood} and \emph{evidence approximation}.

\subsection{Original Training Algorithm}
The original training algorithm in \cite{tipping2001} is derived by setting the derivatives of (\ref{rvm:ML}) to zero.
We obtain the following update equations for $\bm\alpha$ and $\sigma^2$:
\begin{equation}
  \label{rvm:alpha}
  \alpha_j^{\mbox{new}} = \frac{\gamma_j}{\mu_j^2}
\end{equation}
\begin{equation}
  \label{rvm:beta}
  (\sigma^2)^{\mbox{new}} = \frac{||\bm y - \bm\Phi\bm\mu||^2}{N - \sum_j\gamma_j}
\end{equation}
where $\mu_j$ is the $j$th component of the posterior mean $\bm\mu$ (\ref{rvm:mu}).
The quantities $\gamma_j$ are defined by 
\begin{equation}
  \label{rvm:gamma}
  \gamma_j = 1 - \alpha_j \Sigma_{jj}
\end{equation}
where $\Sigma_{jj}$ is the $j$th diagonal element of the posterior covariance $\bm\Sigma$ (\ref{rvm:Sigma}).

To train the model, we can start by giving $\bm\alpha$ and $\sigma^2$ some initial values and evaluate the mean and covariance of the weights posterior using equations (\ref{rvm:mu}) and (\ref{rvm:Sigma}), respectively.
Next we alternate between re-estimating the hyperparamaters $\bm\alpha$ and $\sigma^2$ using (\ref{rvm:alpha}) and (\ref{rvm:beta}) and updating the posterior mean and covariance parameters using (\ref{rvm:mu}) and (\ref{rvm:Sigma}).
We continue until a relevant convergence criterion is met.
For example, we may choose to stop if the change in the marginal likelihood - or, alternatively, the change in the parameter values - between two iterations is below a certain pre-defined threshold.

This procedure is summarised in Algorithm (\ref{rvm:alg1}).
\begin{algorithm}
  \caption{Sparse Bayesian Learning: Original Training Algorithm}
  \label{rvm:alg1}
  \begin{algorithmic}[1]
    \State Choose some initial positive values for $\sigma^2$ and $\alpha_j$ for $j=1,\cdots,M$ 
    \Repeat
    \State $\bm A = \diag(\bm\alpha)$
    \State $\bm\Sigma = \left(\sigma^{-2}\bm\Phi^T\bm\Phi + \bm A\right)^{-1}$
    \State $\bm\mu = \sigma^{-2}\bm\Sigma\bm\Phi^T\bm y$
    \Statex
    \For {$j=1,\cdots,M$}
    \State $\gamma_j = 1 - \alpha_j \Sigma_{jj}$
    \State $\alpha_j = \gamma_j/\mu_j^2$
    \EndFor
    \State $\sigma^2 =||\bm y - \bm\Phi\bm\mu||^2 / (N - \sum_j\gamma_j)$
    \Until Convergence
  \end{algorithmic}
\end{algorithm}

During training, it is typically observed that many of the hyperparameters $\alpha_j$ tend to infinity.
Equations (\ref{rvm:mu}) and (\ref{rvm:Sigma}) imply that the weights $w_j$ corresponding to these hyperparameters have a posterior distribution where the mean and the variance are both zero, meaning their posterior is infinitely peaked at zero.
As a consequence, the corresponding basis functions $\phi_j(\cdot)$ are effectively removed from the model and we achieve sparsity.

In the case of the RVM, where $\phi_j(\cdot) \equiv K(\cdot, \bm x^{(j)})$, the input vectors $\bm x^{(j)}$ corresponding to the remaining non-zero weights are known as the \emph{relevance vectors} of the model.

\subsection{Sequential Sparse Bayesian Learning Algorithm}
A central drawback of the training algorithm discussed in the previous section is its speed. 
The computational complexity scales with the cube of the number of basis functions.
During training, as basis functions are pruned from the model, the algorithm accelerates.
Nevertheless, if $M$ is very large, the procedure can be very expensive to run.

An alternative strategy of maximising the marginal likelihood (\ref{rvm:ML}) was developed by \cite{tipping2003}, resulting in a highly accelerated training algorithm: the \emph{Sequential Sparse Bayesian Learning Algorithm}.
It starts with a single basis function and maximises the marginal likelihood by sequentially adding and deleting candidate basis functions.

To derive the algorithm, we follow the analysis in \cite{tipping2002} and consider the dependence of the marginal likelihood $\mathcal{L}(\bm\alpha,\sigma^2)$ on a single hyperparameter $\alpha_j$.
First, we decompose the matrix $\bm C$, defined in (\ref{rvm:C}), as follows:
\begin{equation*}
  \begin{split}
    \bm C &= \sigma^2 \bm I_N + \sum_{m \neq j} \alpha_m^{-1}\bm\phi_m\bm\phi_m^T + \alpha_j^{-1}\bm\phi_j\bm\phi_j^T \\
    &= \bm C_{-j} + \alpha_j^{-1}\bm\phi_j\bm\phi_j^T
  \end{split}
\end{equation*}
where $\bm C_{-j} \equiv \sigma^2 \bm I_N + \sum_{m \neq j} \alpha_m^{-1}\bm\phi_m\bm\phi_m^T$ is $\bm C$ without the contribution of the $j$th basis vector $\bm\phi_j$.
Making use of standard identities [WHICH ONES?] for matrix inverses and determinants, we can express $|\bm C|$ and $\bm C^{-1}$ as
\begin{equation*}
  \bm C^{-1} = \bm C_{-j}^{-1} - \frac{\bm C_{-j}^{-1}\bm\phi_j\bm\phi_j^T\bm C_{-j}^{-1}}{\alpha_j + \bm\phi_j^T\bm C_{-j}^{-1}\bm\phi_j}
\end{equation*}
\begin{equation*}
  \left|\bm C\right| = \left|\bm C_{-j}\right|\left|1 + \alpha^{-1}_j\bm\phi_j^T\bm C_{-j}^{-1}\bm\phi_j\right|
\end{equation*}

This allows us to decompose the marginal likelihood:
\begin{equation}
  \label{rvm:fastML}
  \begin{split}
    \mathcal{L}(\bm\alpha,\sigma^2) &= \mathcal{L}(\bm\alpha_{-j},\sigma^2) + \frac{1}{2}\left[\log\alpha_j
      - \log(\alpha_j + s_j) + \frac{q_j^2}{\alpha_j + s_j}\right]\\
    &\equiv \mathcal{L}(\bm\alpha_{-j},\sigma^2) + \ell(\alpha_j,\sigma^2)
  \end{split}
\end{equation}
This conveniently seperates terms in $\alpha_j$ in $\ell(\alpha_j,\sigma^2)$ from the remaining terms in $\mathcal{L}(\bm\alpha_{-j},\sigma^2)$, which is the (log) marginal likelihood with the basis vector $\bm\phi_j$ excluded.

The quantity $s_j$ is the \emph{sparsity factor}, defined as
\begin{equation*}
  s_j = \bm\phi_j^T\bm C_{-j}^{-1}\bm\phi_j.
\end{equation*}
It serves as a measure of how much the marginal likelihood would decrease if we added $\bm\phi_j$ to the model.
The quantity $q_j$, on the other hand, is known as the \emph{quality factor}.
It is defined as
\begin{equation*}
  q_j = \bm\phi_j^T\bm C_{-j}^{-1} \bm y
\end{equation*}
and measures the extent to which $\phi_j$ increases $\mathcal{L}(\bm\alpha,\sigma^2)$ by helping to explain the data $\bm y$.
Thus, a particular basis vector $\bm\phi_j$ should not be included in the model if its sparsity factor $s_j$ is large, unless it is offset by a large quality factor $q_j$.

We can see this more explicitly if we consider the first derivative of $\ell(\alpha_j,\sigma^2)$ with respect to $\alpha_j$ \cite{tipping2002}
\begin{equation*}
  \frac{\partial\ell(\alpha_j,\sigma^2)}{\partial\alpha_j} = \frac{\alpha_j^{-1}s_j^2 - (q_j^2 - s_j)}{2(\alpha_j+s_j)^2}
\end{equation*}
Equating it to zero (and noting that $\alpha_j$ is an inverse-variance and therefore positive), we obtain the following solution for $\alpha_j$
\begin{equation}
  \label{rvm:solution}
  \alpha_j = \left\{\begin{array}{lr}
      s_j^2 / (q_j^2 - s_j) & \mbox{if $q_j^2 > s_j$}\\
      +\infty & \mbox{otherwise}
    \end{array}\right..
\end{equation}


\section{Making Predictions}
sparse mu
predictive distribution

For details on the RVM and its implementation see \cite{pilikos2014,tipping2003,tipping2001}.

