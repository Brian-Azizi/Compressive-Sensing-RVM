\chapter{Sparse Bayesian Learning}
\label{ch:rvm}
\emph{Sparse Bayesian Learning} \cite{tipping2001} is a general Bayesian framework within supervised Machine Learning. 
It can be applied to both regression and classification tasks.

In this chapter, we will derive the Sparse Bayesian Learning model for regression.
We will discuss model inference and summarise both the original inference algorithm \cite{tipping2001} and also the faster ``Sequential Sparse Bayesian Learning Algorithm'' \cite{tipping2003}.

The \emph{Relevance Vector Machine}, or \emph{RVM}, is a particular specialisation of the Sparse Bayesian Learning model which has identical functional form to the Support Vector Machine (SVM).
However, the RVM comes with a number of key advantages over the SVM. 
The solution produced by a RVM is typically much sparser than the solution by a comparable SVM.
Furthermore, the RVM is a probabilistic model and as such, allows us to estimate error bounds for its predictions.

Throughout the thesis, we are only interested in the Sparse Bayesian Learning model.
The terms ``Sparse Bayesian Learning'' and ``RVM'' will be used interchangibly,\footnote{Escept for the previous paragraph.}
but they always refer to the more general Sparse Bayesian Learning model.

\section{Model Specification}
We are given a data set of $N$ input vectors $\{\bm x^{(i)}\}^N_{i=1}$ and their associated \emph{targets} $\{y^{(i)}\}_{i=1}^N$.
The input vectors live in $D$-dimensional space, $\bm x \in \mathbb{R}^D$.
The targets are real values, $y \in \mathbb{R}$.
\footnote{When using the Sparse Bayesian model for regression, we assume the targets are real-valued.
  It is also possible to use the model for classification in which case the targets are assumed to be discrete class labels.
}

We model the data using a linearly-weighted sum of $M$ fixed basis functions $\{\phi_j(\cdot)\}_{j=1}^M$ and base our predictions on the function $f(\cdot)$ defined as
\begin{equation} 
  \label{rvm:function}
  f(\bm x; \bm w) = \sum_{j=1}^M w_j \phi_j(\bm x) = \bm w^T \bm \phi(\bm x)
\end{equation}
where $\bm w = [w_1, \dots, w_M]^T$ and $\bm \phi(\cdot) = [\phi_1(\cdot), \dots, \phi_M(\cdot)]^T$.
Using a large number $M$ of non-linear basis functions $\phi_j : \mathbb{R}^D \to \mathbb{R}$ allows for a highly flexible model. 

To train the model (\ref{rvm:function}), i.e. find values for $\bm w$ that are optimal in some sense, we make the standard assumption that our training data are samples from the model with additive noise:
\begin{equation}
  \label{rvm:lklhd_1}
  \begin{split}
    y^{(i)} &= f(\bm x^{(i)}; \bm w) + \epsilon^{(i)}\\
    &= \bm w^T \bm \phi(\bm x^{(i)}) + \epsilon^{(i)}\qquad i = 1, \dots, N.
  \end{split}
\end{equation}
The errors $\{\epsilon^{(i)}\}_{i=1}^N$ are assumed to be independent samples from a zero-mean Gaussian distribution with variance $\sigma^2$
\begin{equation}
  \label{rvm:error}
  p(\epsilon^{(i)}) = \mathcal{N}(\epsilon^{(i)}\,|\,0, \sigma^2)\qquad i = 1, \dots, N.
\end{equation}

Combining equation (\ref{rvm:lklhd_1}) with equation (\ref{rvm:function}), we may express the model for the complete data using matrix notation:
\begin{equation}
  \label{rvm:lklhd_2}
  \bm y = \bm\Phi \bm w + \bm \epsilon
\end{equation}
where $\bm \epsilon = \left[\epsilon^{(1)}, \cdots, \epsilon^{(N)}\right]^T$.
The $N\times M$ matrix $\bm \Phi$ is known as the design matrix. 
The $i$th row of $\bm \Phi$ is given by $\bm \phi(\bm x^{(i)})^T$.
The $j$th column of $\bm \Phi$ is given by $\bm \phi_j = \left[\phi_j\left(\bm x^{(1)}\right), \cdots, \phi_j\left(\bm x^{(N)}\right)\right]^T$, which is also referred to as the $j$th \emph{basis vector}.
Thus
\begin{equation*}
  \bm\Phi =
  \begin{bmatrix}
    \bm\phi_1 & \cdots & \bm\phi_M
  \end{bmatrix}
  =
  \begin{bmatrix}
    \bm\phi(\bm x^{(1)})^T\\
    \vdots\\
    \bm\phi(\bm x^{(N)})^T
  \end{bmatrix}
\end{equation*}

Combining equation (\ref{rvm:lklhd_2}) and equation (\ref{rvm:error}), we find that the complete data likelihood function is given by
\begin{equation}
  \label{rvm:complete_lk}
  \begin{split}
    p\left(\bm y\,|\,\bm w,\sigma^2\right) &= \mathcal{N}\left(\bm y\,|\,\bm w,\sigma^2 \bm I_M\right)\\
    &= (2\pi\sigma^2)^{-N/2} \exp\left\{-\frac{1}{2\sigma^2}||\bm y - \bm\Phi\bm w||^2\right\}
  \end{split}
\end{equation}
where $\bm I_M$ is the $M\times M$ identity matrix.

So far, we have specified the general linear regression model.
To get to the sparse Bayesian formulation, we define a zero-mean Gaussian prior distribution over the parameters $\bm w$
\begin{equation}
\label{eqn:prior_w}
  p(\bm w\,|\,\bm \alpha) = \prod_{j=1}^M \mathcal{N}\left(w_j\,|\,0,\alpha_j^{-1}\right)
\end{equation}
where $\bm \alpha = \left[\alpha_1,\cdots,\alpha_M\right]^T$ is a vector of $M$ hyperparameters.
It is important to note that each hyperparameter $\alpha_j$ is solely responsible for controlling the strength of the prior of its associated weight $w_j$.
If $\alpha_j$ is large, the prior over $w_j$ is very strongly peaked at zero.
This form of the prior distribution is, more than anything, responsible for the dramatic sparsity in the final model.

To complete the specification, we must define a prior over the noise parameter $\sigma^2$ and a hyperprior over the hyperparameters $\bm \alpha$.
Following the derivation in \cite{tipping2001}, we use the following Gamma
\footnote{
The Gamma distribution is defined by   
\[
\GammaDist(z\,|\,a,b) = \Gamma(a)^{-1} b^a z^{a-1} \exp(-bz)  \qquad z,a,b > 0
\]
where $\Gamma(.)$ is the Gamma function defined by 
\[
\Gamma(z) = \int_0^\infty t^{z-1} \exp(-t) dt.
\]
} priors
\begin{equation}
\label{eqn:prior_alpha}
  p(\bm\alpha\,|\,a,b) = \prod_{j=1}^M \GammaDist(\alpha_j\,|\,a,b)
\end{equation}
\begin{equation}
\label{eqn:prior_beta}
  p(\beta\,|\,c,d) = \GammaDist(\beta\,|\,c,d)
\end{equation}
where $\beta \equiv \sigma^{-2}$.

As a side note, consider the prior of $\bm w$ after marginalising out the dependence on the hyperpriors $\bm\alpha$.
Since each $w_j$ is normally distributed with an unknown precision parameter $\alpha_j$ and since the (hyper)prior over $\alpha_j$ is the Gamma distribution and therefore conjugate to $p(w_j\,|\,\alpha_j)$, it follows that the resulting integral can be evaluated analytically
\begin{equation*}
  \begin{split}
    p(\bm w\,|\,a,b) &= \int p(\bm w\,|\,\bm\alpha) p(\bm\alpha\,|\,a,b)d\bm\alpha\\
    &= \prod_{j=1}^M \int \mathcal{N}(w_j\,|\,0,\alpha_j^{-1}) \GammaDist(\alpha_j|\,a,b)d\alpha_j\\
    &= \prod_{j=1}^M \frac{b^a\Gamma(a+\frac{1}{2})}{(2\pi)^{\frac{1}{2}}\Gamma(a)}\left(b+\frac{w_j^2}{2}\right)^{-(a+\frac{1}{2})}.
\end{split}
\end{equation*}
This corresponds to a product of independent Student-t density functions over the weights $w_j$.
For small positive values of $a$ and $b$, the distribution $p(\bm w\,|\,a,b)$ will be strongly peaked at zero.
As discussed in \cite{tipping2001}, it is this hierarchical formulation of the weight prior that is ultimately responsible for encouraging sparse solutions.

\section{Model Inference}
We have specified the likelihood model for the data and a prior distribution over the model parameters.
The next step in Bayesian inference is to compute the posterior distribution of the parameters.
We begin by setting up Bayes' Rule
\begin{equation}
\label{rvm:bayes1}
  p(\bm w, \bm \alpha, \sigma^2 \,|\,\bm y) = \frac{p(\bm y \,|\, \bm w, \bm \alpha, \sigma^2) p(\bm w, \bm \alpha, \sigma^2)}
  {\int p(\bm y \,|\, \bm w, \bm \alpha, \sigma^2) p(\bm w, \bm \alpha, \sigma^2)d\bm w d\bm \alpha d\sigma^2}
\end{equation}

The integral in the denominator of (\ref{rvm:bayes1}) is computationally intractable and we must resort to an alternative strategy.
First, we decompose the left-hand-side of equation (\ref{rvm:bayes1}) as
\begin{equation*}
  p(\bm w, \bm \alpha, \sigma^2\,|\,\bm y) = p(\bm w\,|\,\bm y, \bm \alpha, \sigma^2) p(\bm\alpha,\sigma^2\,|\,\bm y).
\end{equation*}
Next, we use Bayes' Rule to compute the posterior distribution of the weights given $\bm\alpha$ and $\sigma^2$
\begin{equation}
  \label{rvm:bayes2}
  p(\bm w\,|\,\bm y, \bm \alpha, \sigma^2) = \frac{p(\bm y\,|\,\bm w,\sigma^2) p(\bm w\,|\,\bm \alpha)}{p(\bm y\,|\,\bm\alpha,\sigma^2)}
\end{equation}

The denominator of the right-hand-side is known as the \emph{marginal likelihood} and given by
\begin{equation}
  \label{rvm:ML1}
  p(\bm y\,|\,\bm\alpha,\sigma^2) = \int p(\bm y\,|\,\bm w,\sigma^2) p(\bm w\,|\,\bm \alpha) d\bm w
\end{equation}
Since $\bm\alpha$ and $\sigma^2$ are treated as fixed quantities in equation (\ref{rvm:bayes2}), the Gaussian density $p(\bm w\,|\,\bm\alpha)$ is the conjugate prior to the Gaussian likelihood function $p(\bm y\,|\,\bm w,\sigma^2)$.
Thus, the integral in equation (\ref{rvm:ML1}) is a convolution of two Gaussians and therefore equal to another Gaussian:
\begin{equation*}
  \begin{split}
    p(\bm y\,|\,\bm\alpha,\sigma^2) &= \mathcal{N}(\bm y\,|\,\bm 0,\bm C)\\
    &= (2\pi)^{-N/2} |\bm C|^{-1/2} \exp\left\{-\frac{1}{2}\bm y^T \bm C^{-1} \bm y\right\}
  \end{split}
\end{equation*}
where
\begin{equation}
  \label{rvm:C}
  \bm C = \sigma^2 \bm I_N + \bm\Phi\bm A^{-1}\bm\Phi^T.
\end{equation}

The posterior distribution for $\bm w$ is a also a Gaussian:
\begin{equation}
  \label{rvm:posterior}
  p(\bm w\,|\,\bm y,\bm\alpha,\sigma^2) = \mathcal{N}(\bm w\,|\,\bm\mu,\bm\Sigma).
\end{equation}
Its mean $\bm \mu$ and covariance matrix $\bm\Sigma$ are given by
\begin{equation}
  \label{rvm:Sigma}
  \bm\Sigma = \left(\sigma^{-2}\bm\Phi^T\bm\Phi + \bm A\right)^{-1}
\end{equation}
\begin{equation}
  \label{rvm:mu}
  \bm\mu = \sigma^{-2}\bm\Sigma\bm\Phi^T\bm y
\end{equation}
with $\bm A = \diag(\bm\alpha)$.

Thus, to train the model, we need to find the hyperparameters $\bm \alpha$ and $\sigma^2$.
In the context of the RVM, these hyperparameters are usually estimated from the data by performing a maximization of the marginal likelihood, or equivalently, its logarithm
\begin{equation}
  \label{rvm:ML}
  \begin{split}
    \mathcal{L}(\bm\alpha,\sigma^2) &= \log p(\bm y\,|\,\bm\alpha,\sigma^2) = \log \mathcal{N}(\bm y \,|\,\bm 0, \bm C)\\
    &=-\frac{1}{2}\left[N\log 2\pi + \log|\bm C| + \bm y^T\bm C^{-1} \bm y\right]
\end{split}    
\end{equation}

\subsection{Original Training Algorithm}
\label{sect:rvm_orig}
The original training algorithm in \cite{tipping2001} can be obtained by setting the derivatives of (\ref{rvm:ML}) to zero and rearring to get the following update equations for $\bm\alpha$ and $\sigma^2$:
\begin{equation}
  \label{rvm:alpha}
  \alpha_j^{\mbox{new}} = \frac{\gamma_j}{\mu_j^2}
\end{equation}
\begin{equation}
  \label{rvm:beta}
  (\sigma^2)^{\mbox{new}} = \frac{||\bm y - \bm\Phi\bm\mu||^2}{N - \sum_j\gamma_j}
\end{equation}
where $\mu_j$ is the $j$th component of the posterior mean $\bm\mu$ (\ref{rvm:mu}).
The quantities $\gamma_j$ are defined by 
\begin{equation*}
  \gamma_j = 1 - \alpha_j \Sigma_{jj}
\end{equation*}
where $\Sigma_{jj}$ is the $j$th diagonal element of the posterior covariance $\bm\Sigma$ (\ref{rvm:Sigma}).

To train the model, we can start by giving $\bm\alpha$ and $\sigma^2$ some initial values and evaluate the mean and covariance of the weights posterior using equations (\ref{rvm:mu}) and (\ref{rvm:Sigma}), respectively.
Next we alternate between re-estimating the hyperparamaters $\bm\alpha$ and $\sigma^2$ using (\ref{rvm:alpha}) and (\ref{rvm:beta}) and updating the posterior mean and covariance parameters using (\ref{rvm:mu}) and (\ref{rvm:Sigma}).
We continue until a relevant convergence criterion is met.
For example, we may choose to stop if the change in the marginal likelihood between two consecutive iterations is below a certain pre-defined threshold.

This procedure is summarised in Algorithm \ref{rvm:alg1}.
\begin{algorithm}
  \caption{Sparse Bayesian Learning: Original Training Algorithm}
  \label{rvm:alg1}
  \begin{algorithmic}[1]
    \State Choose some initial positive values for $\sigma^2$ and $\alpha_j$ for $j=1,\cdots,M$ 
    \Repeat
    \State $\bm A = \diag(\bm\alpha)$
    \State $\bm\Sigma = \left(\sigma^{-2}\bm\Phi^T\bm\Phi + \bm A\right)^{-1}$
    \State $\bm\mu = \sigma^{-2}\bm\Sigma\bm\Phi^T\bm y$
    \Statex
    \For {$j=1,\cdots,M$}
    \State $\gamma_j = 1 - \alpha_j \Sigma_{jj}$
    \State $\alpha_j = \gamma_j/\mu_j^2$
    \EndFor
    \State $\sigma^2 =||\bm y - \bm\Phi\bm\mu||^2 / (N - \sum_j\gamma_j)$
    \Until Convergence
  \end{algorithmic}
\end{algorithm}

During training, it is typically observed that many of the hyperparameters $\alpha_j$ tend to infinity.
Equations (\ref{rvm:mu}) and (\ref{rvm:Sigma}) imply that the weights $w_j$ corresponding to these hyperparameters have a posterior distribution where the mean and the variance are both zero, meaning their posterior is infinitely peaked at zero.
As a consequence, the corresponding basis functions $\phi_j(\cdot)$ are effectively removed from the model and we achieve sparsity.

\subsection{Sequential Sparse Bayesian Learning Algorithm}
\label{sect:ssbl}
A central drawback of the training algorithm discussed in the previous section is its speed. 
The computational complexity scales with the cube of the number of basis functions.
During training, as basis functions are pruned from the model, the algorithm accelerates.
Nevertheless, if $M$ is very large, the procedure can be very expensive to run.

An alternative strategy of maximising the marginal likelihood (\ref{rvm:ML}) was developed by \cite{tipping2003}, resulting in a highly accelerated training algorithm: the \emph{Sequential Sparse Bayesian Learning Algorithm} (SSBL).
It starts with a single basis function and maximises the marginal likelihood by sequentially adding, re-estimating or deleting candidate basis functions.
This significantly reduces the computational complexity of the algorithm.

To derive the algorithm, we follow the analysis in \cite{tipping2002} and consider the dependence of the marginal likelihood $\mathcal{L}(\bm\alpha,\sigma^2)$ on a single hyperparameter $\alpha_j$.
First, we decompose the matrix $\bm C$, defined in (\ref{rvm:C}), as follows:
\begin{equation*}
  \begin{split}
    \bm C &= \sigma^2 \bm I_N + \sum_{m \neq j} \alpha_m^{-1}\bm\phi_m\bm\phi_m^T + \alpha_j^{-1}\bm\phi_j\bm\phi_j^T \\
    &= \bm C_{-j} + \alpha_j^{-1}\bm\phi_j\bm\phi_j^T
  \end{split}
\end{equation*}
where $\bm C_{-j} \equiv \sigma^2 \bm I_N + \sum_{m \neq j} \alpha_m^{-1}\bm\phi_m\bm\phi_m^T$ is $\bm C$ without the contribution of the $j$th basis vector $\bm\phi_j$.
Making use of the Woodbury identity and the matrix determinant lemma, we can express $|\bm C|$ and $\bm C^{-1}$ as
\begin{equation*}
  \bm C^{-1} = \bm C_{-j}^{-1} - \frac{\bm C_{-j}^{-1}\bm\phi_j\bm\phi_j^T\bm C_{-j}^{-1}}{\alpha_j + \bm\phi_j^T\bm C_{-j}^{-1}\bm\phi_j}
\end{equation*}
\begin{equation*}
  \left|\bm C\right| = \left|\bm C_{-j}\right|\left|1 + \alpha^{-1}_j\bm\phi_j^T\bm C_{-j}^{-1}\bm\phi_j\right|
\end{equation*}

This allows us to decompose the marginal likelihood:
\begin{equation}
  \label{rvm:fastML}
  \begin{split}
    \mathcal{L}(\bm\alpha,\sigma^2) &= \mathcal{L}(\bm\alpha_{-j},\sigma^2) + \frac{1}{2}\left[\log\alpha_j
      - \log(\alpha_j + s_j) + \frac{q_j^2}{\alpha_j + s_j}\right]\\
    &\equiv \mathcal{L}(\bm\alpha_{-j},\sigma^2) + \ell(\alpha_j,\sigma^2)
  \end{split}
\end{equation}
This conveniently separates terms in $\alpha_j$ in $\ell(\alpha_j,\sigma^2)$ from the remaining terms in $\mathcal{L}(\bm\alpha_{-j},\sigma^2)$, which is the (log) marginal likelihood with the basis vector $\bm\phi_j$ excluded.

\begin{figure}
  \centering
  \begin{tikzpicture}[scale=2]
    \draw[thick,<-] (0,2) node [above] {$\ell(\alpha_j,\sigma^2)$} -- (0,0);
    \draw[thick,->] (0,0) -- (3,0) node [below] {$\alpha_j$};
    \draw[thick,<-] (4,2) node [above] {$\ell(\alpha_j,\sigma^2)$} -- (4,0);
    \draw[thick,->] (4,0) -- (7,0) node [below] {$\alpha_j$};
    \node at (1.9,0.8) {$q_j^2 > s_j$};
    \node at (6,0.8) {$q_j^2 < s_j$};
    \draw[thick] (0,0.2) to [out=60,in=230] (0.8,1.7) to [out=50,in=130] (1.2,1.7) to [out=310,in=170] (2,1.2) to [out=350,in=178] (2.8,1.1);
    \draw[dashed] (0.95,1.75) -- (0.95,0) node [below] {$\frac{s^2_j}{q_j^2-s_j}$};
    \draw[thick] (4,0.2) to [out=55,in=230] (5,1.4) to [out=50,in=183] (6.8,1.9);
    \draw[thin] (0.15,0.05) -- (0.15,0) node[below] {$_{10^{-7}}$};
    \draw[thin] (1.45,0.05) -- (1.45,0) node[below] {$_{10^0}$};
    \draw[thin] (2.7,0.05) -- (2.7,0) node[below] {$_{10^7}$};
    \draw[thin] (4.15,0.05) -- (4.15,0) node[below] {$_{10^{-7}}$};
    \draw[thin] (5.45,0.05) -- (5.45,0) node[below] {$_{10^0}$};
    \draw[thin] (6.7,0.05) -- (6.7,0) node[below] {$_{10^7}$};
  \end{tikzpicture}
  \caption[Plots of the Marginal Likelihood Function]{Example plots of $\ell(\alpha_j,\sigma^2)$ against $\alpha_j$ illustrating the stationary points when $q_j^2>s_j$ (left) and $q_j^2<s_j$ (based on \cite{tipping2002}).}
  \label{fig:ml_plot}
\end{figure}

The quantity $s_j$ is the \emph{sparsity factor}, defined as
\begin{equation*}
  s_j = \bm\phi_j^T\bm C_{-j}^{-1}\bm\phi_j.
\end{equation*}
It serves as a measure of how much the marginal likelihood would decrease if we added $\bm\phi_j$ to the model.
The quantity $q_j$, on the other hand, is known as the \emph{quality factor}.
It is defined as
\begin{equation*}
  q_j = \bm\phi_j^T\bm C_{-j}^{-1} \bm y
\end{equation*}
and measures the extent to which $\phi_j$ increases $\mathcal{L}(\bm\alpha,\sigma^2)$ by helping to explain the data $\bm y$.
Thus, a particular basis vector $\bm\phi_j$ should not be included in the model if its sparsity factor $s_j$ is large, unless it is offset by a large quality factor $q_j$.

We can see this more explicitly if we consider the first derivative of $\ell(\alpha_j,\sigma^2)$ with respect to $\alpha_j$ \cite{tipping2002}
\begin{equation*}
  \frac{\partial\ell(\alpha_j,\sigma^2)}{\partial\alpha_j} = \frac{\alpha_j^{-1}s_j^2 - (q_j^2 - s_j)}{2(\alpha_j+s_j)^2}
\end{equation*}
Equating it to zero (and noting that $\alpha_j$ is an inverse-variance and therefore positive), we obtain the following solution for $\alpha_j$:
\begin{equation}
  \label{rvm:solution}
  \alpha_j = \left\{\begin{array}{lr}
      s_j^2 / (q_j^2 - s_j) & \mbox{if $q_j^2 > s_j$}\\
      +\infty & \mbox{otherwise}
    \end{array}\right..
\end{equation}
The solution (\ref{rvm:solution}) is illustrated in Figure \ref{fig:ml_plot}.

It follows that, if, during training, a candidate basis vector $\bm\phi_j$ is currently included in the model (meaning $\alpha_j < \infty$) even though $q_j^2 \leq s_j$, then $\alpha_j$ should be set to $\infty$ and $\bm\phi_j$ should be pruned from the model.
On the other hand, if $\bm\phi_j$ is currently excluded from the model (i.e. $\alpha_j=\infty$), but $q_j^2 > s_j$, then $\alpha_j$ should be set to $s_j^2/(q_j^2-s_j)$ and $\bm\phi_j$ should be added to the model.
Furthermore, if $\bm\phi_j$ is included and $q_j^2 > s_j$, then we may also re-estimate $\alpha_j$.
Each step in the algorithm increases the marginal likelihood. 
Thus we are guaranteed to find a maximum.

During the algorithm, we must maintain and update values of the quality factors and sparsity factors for all basis functions, as well as the posterior mean $\bm\mu$ and covariance $\bm\Sigma$ of the weights $\bm w$.
In practice, it easier to keep track of the quantities $Q_m = \bm\phi_m^T\bm C^{-1}\bm\phi_m$ and $S_m = \bm\phi_m^T\bm C^{-1}\bm y$ which can also be written as (using the Woodbury Identity)
\begin{equation}
  \label{rvm:S}
  S_m = \sigma^{-2}\bm\phi_m^T\bm\phi_m - \sigma^{-4}\bm\phi_m^T\bm\Phi\bm\Sigma\bm\Phi^T\bm\phi_m
\end{equation}
\begin{equation}
  \label{rvm:Q}
  Q_m = \sigma^{-2}\bm\phi_m^T\bm y - \sigma^{-4}\bm\phi_m^T\bm\Phi\bm\Sigma\bm\Phi^T\bm y
\end{equation}
where $\bm\Sigma$ and $\bm\Phi$ contain only the basis functions that are currently included in the model.
                                                 
\begin{algorithm}
  \caption{Sequential Sparse Bayesian Learning Algorithm \cite{tipping2003}}
  \label{rvm:alg2}
  \begin{algorithmic}[1]
    \State Initialise $\sigma^2$.\label{rvm:initSig}
    \State Add basis function $\bm\phi_j$ to the model, where $j = \argmax_m \left\{||\bm\phi_m^T\bm y||^2/||\bm\phi_m||^2\right\}$.\label{rvm:initPhi}
    \Statex Set $\alpha_j = \frac{||\bm\phi_j||^2}{||\bm\phi_j^T\bm y||^2/||\bm\phi_j||^2 - \sigma^2}$. Set $\alpha_m = \infty$ for $m\neq j$.
    \State Compute $\bm\Sigma = \left(\sigma^{-2}\bm\Phi^T\bm\Phi + \bm A\right)^{-1}$ and $\bm\mu = \sigma^{-2}\bm\Sigma\bm\Phi^T\bm y$ which are scalars initially.\label{rvm:fullStats}
    \Statex Compute $S_m$, $Q_m$, $s_m$ and $q_m$ for $m = 1,\cdots,M$ using (\ref{rvm:S}) $-$ (\ref{rvm:q}). %(\ref{rvm:Q}),(\ref{rvm:s})
    \Repeat\label{rvm:bigLoop}
    \State Select some candidate basis vector $\bm\phi_j$.\label{rvm:select}
    \IIf{ $q_j^2 > s_j$ and $\alpha_j = \infty$} \textbf{add} $\bm\phi_j$ to the model and update $\alpha_j$. \EndIIf\label{rvm:add}
    \IIf{ $q_j^2 > s_j$ and $\alpha_j < \infty$} \textbf{re-estimate} $\alpha_j$. \EndIIf\label{rvm:reestimate}
    \IIf{ $q_j^2 < s_j$ and $\alpha_j < \infty$} \textbf{delete} $\bm\phi_j$ from the model and set $\alpha_j=\infty$. \EndIIf\label{rvm:delete}
    \State Update $\sigma^2 = ||\bm y - \bm\Phi\bm w||/(N-M+\sum_m\alpha_m\bm\Sigma_{mm})$\label{rvm:noise}\cite{tipping2001}.
    \State Update $\bm\Sigma$, $\bm\mu$ and $S_m$, $Q_m$, $s_m$, $q_m$ for $m = 1,\cdots, M$.\label{rvm:update}
    \Until Convergence
  \end{algorithmic}
\end{algorithm}

The factors $s_m$ and $q_m$ can by obtained from $S_m$ and $Q_m$ as follows:
\begin{equation}
  \label{rvm:s}
  s_m = \frac{\alpha_m S_m}{\alpha_m - S_m}
\end{equation}
\begin{equation}
  \label{rvm:q}
  q_m = \frac{\alpha_m Q_m}{\alpha_m - S_m}
\end{equation}
Note that if $\alpha_m = \infty$, then $q_m = Q_m$ and $s_m = S_m$.

We have summarized the procedure in Algorithm \ref{rvm:alg2}.
After initializing the standard deviation $\sigma^2$ in step \ref{rvm:initSig}, we add the first basis function $\bm\phi_j$ to the model. 
We could initialize with any basis vector, but in step \ref{rvm:initPhi}, we pick the one with the largest normalized projection on the target vector $\bm y$, i.e. we choose  $j = \argmax_m \left\{||\bm\phi_m^T\bm y||^2/||\bm\phi_m||^2\right\}$.
In step \ref{rvm:fullStats} we compute the model statistics and in step \ref{rvm:bigLoop} we begin the large loop of the algorithm.

There are two things to note here. 
First, in step \ref{rvm:select}, we need to select a candidate basis vector $\bm\phi_j$. 
We are free to pick one at random. 
Alternatively, it is possible to compute the change in the marginal likelihood for each candidate basis vector and choose the one that would give us the largest increase.

Second, we would usually like to estimate the noise variance $\sigma^2$ from the data, as is done in step \ref{rvm:noise}.
However, in practice, we may decide to set $\sigma^2$ in advance in step \ref{rvm:initSig} and keep it fixed throughout the algorithm.
If we decide to do so, then we can perform the updates in step \ref{rvm:update} using very efficient update formulae that do not require matrix inversions.
The formulae can be found in the appendix of \cite{tipping2003}.
If we do decide to update $\sigma^2$ in step \ref{rvm:noise}, then we must use the full equations (\ref{rvm:Sigma}), (\ref{rvm:mu}) and (\ref{rvm:S})-(\ref{rvm:q}).
Alternatively, we could also choose to update $\sigma^2$ after a set number of iterations.

\section{Making Predictions}
Once we have trained the model, we may use it to predict the target $y^*$ for a new input vector $\bm x^*$.
To do so, we would like to compute the \emph{predictive distribution}
\begin{equation*}
  p(y^*\,|\,\bm y) = \int p(y^*\,|\,\bm w, \bm\alpha,\sigma^2) p(\bm w,\bm\alpha,\sigma^2\,|\,\bm y)\,d\bm w \,d\bm\alpha \,d\sigma^2.
\end{equation*}

We cannot compute this integral analytically, nor do we actually know the posterior of all the model parameters.
Instead, we use the type-II maximum likelihood solutions for $\bm\alpha$ and $\sigma^2$ that we obtained during training and base our predictions on the posterior distribution of the weights conditioned on $\bm\alpha$ and $\sigma^2$. 
The predictive distribution for $y^*$ is then:
\begin{equation}
  \label{rvm:predictive}
  p(y^*\,|\,\bm y,\bm\alpha, \sigma^2) = \int p(y^*\,|\,\bm w, \sigma^2) p(\bm w\,|\,\bm y,\bm\alpha,\sigma^2)\,d\bm w
\end{equation}

Both factors in the integrand are Gaussians, and we can therefore readily compute the integral to get
\begin{equation}
  p(y^*\,|\,\bm y, \bm\alpha,\sigma^2) = \mathcal{N}(y^*\,|\,\mu^*,(\sigma^2)^*)
\end{equation}
The predictive mean is given by
\begin{equation}
  \label{rvm:pred_mean}
  \mu^* = \bm\mu^T\bm\phi(\bm x^*)
\end{equation}
and the predictive variance is given by
\begin{equation}
  \label{rvm:pred_var}
  (\sigma^2)^* = \sigma^2 + \bm\phi(\bm x^*)^T\bm\Sigma\bm\phi(\bm x^*)
\end{equation}

Equation (\ref{rvm:pred_mean}) implies that, if we want to produce point predictions, we may simply set the weights $\bm w$ equal to posterion mean $\bm\mu$ which is typically very sparse.

If we are also interested in error bars for our predictions, we can obtain them using Equation (\ref{rvm:pred_var}).
The error bars consist of two parts, the noise in the data $\sigma^2$ and the uncertainty in the weights.
We may need to be careful when using the error bars as they become inreliable the further we move away from the training set \cite{rasmussen2005}. 

For more details and derivations on Sparse Bayesian Learning, see \cite{tipping2001,tipping2002,tipping2003}.

