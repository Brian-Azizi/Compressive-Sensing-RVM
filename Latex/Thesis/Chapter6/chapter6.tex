\chapter{Implementation Details and Code optimization}
This chapter gives a brief description of the current state of our implementation of the 3D signal reconstructer.

\section{Haar basis functions}
The RVM takes as input a target vector ($\bs y$) and a basis matrix ($\bs \Psi$). 
In this respect, it is agnostic about whether the signal is an image or video or of some other type alltogether.
Most of this information is encoded in the basis matrix $\bs \Psi$.
It is therefore important, and often challenging, to select a good set of basis functions.

Our current implementation uses 3-dimensional Haar wavelet basis functions.
I will show how the basis matrix $\bs\Psi$ is constructed by briefly describing how the discrete Haar wavelet transform is performed on 1D, 2D and finally on 3D signals.

\subsection{1D Haar wavelet transform}
Consider a 1-dimensional signal $\bs s = \{s_1,\dots,s_r\} \in \mathbb{R}^r$ ($r$ for ``rows''), where, for simplicity, we assume that $r$ is a power of 2.
The Haar wavelet transform can be performed at various resolution scales.
The transform at the first scale is given by:
\begin{equation*}
\bs s = \{s_1,\dots,s_r\}\to \frac{1}{\sqrt{2}}\{s_1+s_2,s_3+s_4,\dots,s_{r-1}+s_r,s_1-s_2,\dots,s_{r-1}-s_r\}=\hat{\bs s}^{(1)}
\end{equation*}
The first half of the signal is replaced by scaled averages of adjacent elements and the second half is replaced by scaled differences of adjacent elements.
By performing this transform again on the first half of $\hat{\bs s}^{(1)}$ while keeping the second half fixed, we get the Haar wavelet transform at the second scale $\hat{\bs s}^{(2)}$. 
To get the third scale transform $\hat{\bs s}^{(3)}$, we perform the initial transform on the first quarter of $\hat{\bs s}^{(2)}$ while keeping the rest of the signal fixed.
We may continue this process until we reach the $i$th scale, where $2^i = r$.

From here on, we will only consider the first scale transform $\hat{\bs s}^{(1)}$ and we will omit the $(1)$ superscript.
We can express the transform as a multiplication by an orthogonal $r\times r$ matrix $W$ given by
\begin{equation}
W = \begin{bmatrix}
  \Phi_r \\
  \Psi_r \\
\end{bmatrix}
\end{equation}
where $\Phi_r$ and $\Psi_r$ are $(r/2)\times r$ matrices\footnote{Note that the matrix $\Psi_r$ used here is different to the matrix $\Psi$ that was used in the previous chapter (which corresponds to $W^T$ here).} given by
\begin{equation*}
\Phi_r = \frac{1}{\sqrt{2}} \begin{pmatrix}
1&1&0&0&\cdots&0&0\\
0&0&1&1&\cdots&0&0\\
\vdots&\vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\
0&0&0&0&\cdots&1&1
\end{pmatrix}
\end{equation*}
and 
\begin{equation*}
\Psi_r = \frac{1}{\sqrt{2}} \begin{pmatrix}
1&-1&0&0&\cdots&0&0\\
0&0&1&-1&\cdots&0&0\\
\vdots&\vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\
0&0&0&0&\cdots&1&-1
\end{pmatrix}
\end{equation*}


In the signal processing literature, $\Phi_r$ is referred to as a low pass filter, while $\Psi_r$ is referred to as a high pass filter.
$\Phi_r$ outputs an average of the signal and $\Psi_r$ outputs the details of the signal.

\subsection{2D Haar wavelet transform}
Let $A \in \mathbb{R}^{r\times c}$ be a 2-dimensional signal (e.g. an image).
For simplicity, we will assume that both $r$ and $c$ are powers of 2 (though not necessarily equal).

It is simple to obtain $A$'s Haar wavelet transform $\hat{A}$ at the first scale.
This is done by first applying the 1-dimensional transform individually to each column of $A$ to obtain a temporary matrix $\hat A_{temp}$.
Next, we apply the 1-dimensional haar wavelet transform individually to each row of $\hat A_{temp}$ to obtain $\hat A$.

We can again express the transform as a multiplication of matrices:
\begin{equation}
\label{eqn:2Dtransform}
\hat A = \begin{bmatrix}
  \Phi_r\\
  \Psi_r\\
\end{bmatrix}
A
\begin{bmatrix}
  \Phi_c^T & \Psi_c^T\\
\end{bmatrix}
\end{equation}
where $\Phi_r$ and $\Psi_r$ are as before and $\Phi_c$ and $\Psi_c$ are of similar form but each have dimensions $(c/2)\times c$.
This is the transform that was used to generate the RHS of Figure \ref{fig:haarlenna}.
We note that the high-pass filters essentially detect edges of various orientations in the image.

However, as it currently stands, we cannot use this form of the basis transformation for the reconstruction algorithm. Recall that the RVM requires a \emph{vector} of measurements as opposed to a matrix and also that it requires a single basis matrix, not a basis transform as given in (\ref{eqn:2Dtransform}).

To do this, we store the 2-dimensional signal $A$ as a long column vector $\bs a$ of length $rc$ by pasting the individual columns of $A$ one after another.
The basis transformation of $\bs a$ can then be expressed as 
\begin{equation*}
\hat{\bs a} = W \bs a
\end{equation*}
where $W$ is a $rc \times rc $ matrix given by
\begin{equation*}
W = 
\begin{bmatrix}
\Phi_c \otimes \Phi_r \\
\Phi_c \otimes \Psi_r \\
\Psi_c \otimes \Phi_r \\
\Psi_c \otimes \Psi_r \\
\end{bmatrix}
\end{equation*}

The symbol $\otimes$ denotes the \emph{Kronecker product}. 
The kronecker product $P \otimes Q$ between matrices $P$ and $Q$ with dimensions $m_P \times n_P$ and $m_Q \times n_Q$, respectively,  is defined to be the block matrix
\begin{equation*}
\begin{bmatrix}
p_{1,1} Q & p_{1,2} Q & \cdots & p_{1,n_P} Q \\
p_{2,1} Q & p_{2,2} Q & \cdots & p_{2,n_P} Q \\
\vdots&\vdots&\ddots&\vdots \\
p_{m_P,1} Q & p_{m_P,2} Q & \cdots & p_{m_P,n_P} Q \\
\end{bmatrix}
\end{equation*}
of size $m_Pm_Q \times n_Pn_Q$.

\subsection{3D Haar wavelet transform}
Let $V \in \mathbb{R}^{r\times c\times s}$ be a 3-dimensional signal such as a video. 
$V$ has $r$ rows, $c$ columns and $s$ slices, and we assume that $r$, $c$ and $s$ are all powers of 2. 
We may visualize $V$ as a ``volume'' with 2 spacial dimensions and one time dimension corresponding to frames of the video.

To obtain the Haar wavelet transform $\hat V$ of $V$, we first perform the 1-dimensional transform individually on each column in every slice of $V$ to get $\hat V_{temp1}$.
We then perform the 1D transform on every row in every slice of $\hat V_{temp_1}$ to get $\hat V_{temp_2}$.
Finally, we perform the 1D transform across the slices for every row and column to get $\hat V$.

However, like in the 2-dimensional case, we need to be able to pass a single vector of coefficients and a single basis matrix to the RVM.
To do this, we vectorize $V$ as follows. 
First, we vectorize each individual slice of $V$ as before in the 2D case.
Then, we stack all these vectors on top each other to get one very long column vector $\bs v$ of length $rcs$.
The Haar wavelet transform is given by 
\begin{equation*}
\hat{\bs v} = W \bs v
\end{equation*}
where 
\begin{equation*}
W = 
\begin{bmatrix}
\Phi_s \otimes \Phi_c \otimes \Phi_r \\
\Phi_s \otimes \Phi_c \otimes \Psi_r \\
\Phi_s \otimes \Psi_c \otimes \Phi_r \\
\Phi_s \otimes \Psi_c \otimes \Psi_r \\
\Psi_s \otimes \Phi_c \otimes \Phi_r \\
\Psi_s \otimes \Phi_c \otimes \Psi_r \\
\Psi_s \otimes \Psi_c \otimes \Phi_r \\
\Psi_s \otimes \Psi_c \otimes \Psi_r \\
\end{bmatrix}
\end{equation*}
Comparing notation to the previous chapter, what we refer to as $W$ here is the transpose of what was previously denoted as $\Psi$.
And since $\bs v = W^T \hat{\bs v}$, we see that $\bs v$ corresponds to what was previously called $\bs x$. 

\section{Update formulae, details on RVM implementation}